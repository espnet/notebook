{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HANJionghao/espnet_notebook/blob/master/ESPnetEZ/TTS/TTS_finetune_vctk_dump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc_5YHRXb7lB"
      },
      "source": [
        "# Fine-Tuning VITS for Text-to-Speech Synthesis on a New Dataset\n",
        "In this tutorial, we will guide you through the process of performing text-to-speech (TTS) synthesis by fine-tuning the VITS model on the VCTK dataset. This demo covers data preparation from dump files, model fine-tuning, inference, and evaluation.\n",
        "\n",
        "## Overview\n",
        "- Task: Text-to-Speech (TTS)\n",
        "- Dataset: [VCTK](http://www.udialogue.org/download/cstr-vctk-corpus.html)\n",
        "- Model: VITS - [espnet/kan-bayashi_libritts_xvector_vits](https://huggingface.co/espnet/kan-bayashi_libritts_xvector_vits)\n",
        "\n",
        "## License Reminder\n",
        "Before proceeding, please note that the dataset and model used in this tutorial come with specific licensing terms:\n",
        "- **VCTK Corpus:** Licensed under the Open Data Commons Attribution License (ODC-By) v1.0.\n",
        "- **Model:** The pretrained VITS model is under the Creative Commons Attribution 4.0 License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0LaeC3RzECk"
      },
      "source": [
        "# Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1k1-M7LuFID"
      },
      "source": [
        "## Clone ESPnet's Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_H3k5A8uFIE"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/espnet/espnet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KOumQAEuFIE"
      },
      "source": [
        "## Install ESPnet and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_37vomruFIE"
      },
      "outputs": [],
      "source": [
        "# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care\n",
        "# ESPnet installation\n",
        "!git clone --depth 5 https://github.com/espnet/espnet.git\n",
        "!cd espnet && pip install .\n",
        "\n",
        "!pip install espnet_model_zoo tensorboard\n",
        "\n",
        "!pip install pyopenjtalk==0.4\n",
        "!pip install pypinyin==0.44.0\n",
        "!pip install gdown==4.4.0\n",
        "!pip install ipywebrtc\n",
        "\n",
        "# Evaluation related\n",
        "!git clone --depth 5 https://github.com/shinjiwlab/versa.git\n",
        "!cd versa && pip install .\n",
        "!git clone https://github.com/ftshijt/versa_demo_egs.git\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieViDVf1uFIF"
      },
      "source": [
        "## Import ESPnetEZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8qMB12wuFIF"
      },
      "outputs": [],
      "source": [
        "import espnetez as ez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHKwsqGUAT1y"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xow3K7ZXqery"
      },
      "source": [
        "In this tutorial, we will use ESPnet-generated dump files as our inputs. Set up the directory where your processed dump folder is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFPdZ8qguFIF"
      },
      "outputs": [],
      "source": [
        "DUMP_DIR = f\"dump\"\n",
        "data_info = {\n",
        "    \"speech\": [\"wav.scp\", \"sound\"],\n",
        "    \"text\": [\"text\", \"text\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3pm3nJECzlG"
      },
      "source": [
        "# Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgdqEHYTuFIG"
      },
      "source": [
        "## Download Pretrained VITS Model\n",
        "We'll use ESPnet's model zoo to download the [pretrained VITS model from the LibriTTS corpus](https://huggingface.co/espnet/kan-bayashi_libritts_xvector_vits).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU6chQMpuFIG"
      },
      "outputs": [],
      "source": [
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "\n",
        "PRETRAIN_MODEL = \"espnet/kan-bayashi_libritts_xvector_vits\"\n",
        "d = ModelDownloader()\n",
        "pretrain_downloaded = d.download_and_unpack(PRETRAIN_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Sx8kfiuFIG"
      },
      "source": [
        "## Configure Fine-Tuning\n",
        "\n",
        "Load the pretrained model's configuration and set it up for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HiT2QzluFIG"
      },
      "outputs": [],
      "source": [
        "TASK = \"gan_tts\"\n",
        "\n",
        "pretrain_config = ez.config.from_yaml(TASK, pretrain_downloaded[\"train_config\"])\n",
        "\n",
        "# Update the configuration with the downloaded model file path\n",
        "pretrain_config[\"model_file\"] = pretrain_downloaded[\"model_file\"]\n",
        "\n",
        "# Modify configuration for fine-tuning\n",
        "finetune_config = pretrain_config.copy()\n",
        "finetune_config[\"batch_size\"] = 1\n",
        "finetune_config[\"num_workers\"] = 1\n",
        "finetune_config[\"max_epoch\"] = 100\n",
        "finetune_config[\"batch_bins\"] = 500000\n",
        "finetune_config[\"num_iters_per_epoch\"] = None\n",
        "finetune_config[\"generator_first\"] = True\n",
        "\n",
        "# Disable distributed training\n",
        "finetune_config[\"distributed\"] = False\n",
        "finetune_config[\"multiprocessing_distributed\"] = False\n",
        "finetune_config[\"dist_world_size\"] = None\n",
        "finetune_config[\"dist_rank\"] = None\n",
        "finetune_config[\"local_rank\"] = None\n",
        "finetune_config[\"dist_master_addr\"] = None\n",
        "finetune_config[\"dist_master_port\"] = None\n",
        "finetune_config[\"dist_launcher\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOgU7P6duFIG"
      },
      "source": [
        "## Initialize Trainer\n",
        "\n",
        "Define the trainer for the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j153lPhscCr2"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"vctk\"\n",
        "EXP_DIR = f\"./exp/finetune_{TASK}_{DATASET_NAME}\"\n",
        "STATS_DIR = f\"./exp/stats_{DATASET_NAME}\"\n",
        "ngpu = 1\n",
        "\n",
        "trainer = ez.Trainer(\n",
        "    task=TASK,\n",
        "    train_config=finetune_config,\n",
        "    train_dump_dir=f\"{DUMP_DIR}/raw/tr_no_dev\",\n",
        "    valid_dump_dir=f\"{DUMP_DIR}/raw/dev\",\n",
        "    data_info=data_info,\n",
        "    output_dir=EXP_DIR,\n",
        "    stats_dir=STATS_DIR,\n",
        "    ngpu=ngpu,\n",
        ")\n",
        "\n",
        "# Add the xvector paths to the configuration\n",
        "trainer.train_config.train_data_path_and_name_and_type += [\n",
        "    [f\"{DUMP_DIR}/xvector/tr_no_dev/xvector.scp\", \"spembs\", \"kaldi_ark\"],\n",
        "]\n",
        "trainer.train_config.valid_data_path_and_name_and_type += [\n",
        "    [f\"{DUMP_DIR}/xvector/dev/xvector.scp\", \"spembs\", \"kaldi_ark\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v17qZkpGuFIG"
      },
      "source": [
        "## Collect Statistics\n",
        "\n",
        "Before training, we need to collect data statistics (e.g., normalization stats)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwfR9NzKuFIG"
      },
      "outputs": [],
      "source": [
        "# Temporarily set to None, as we need to collect stats first\n",
        "trainer.train_config.normalize = None\n",
        "trainer.train_config.pitch_normalize = None\n",
        "trainer.train_config.energy_normalize = None\n",
        "\n",
        "# Collect stats\n",
        "trainer.collect_stats()\n",
        "\n",
        "# Restore normalization configs with collected stats\n",
        "trainer.train_config.write_collected_feats = False\n",
        "if finetune_config[\"normalize\"] is not None:\n",
        "    trainer.train_config.normalize = finetune_config[\"normalize\"]\n",
        "    trainer.train_config.normalize_conf[\"stats_file\"] = (\n",
        "        f\"{STATS_DIR}/train/feats_stats.npz\"\n",
        "    )\n",
        "if finetune_config[\"pitch_normalize\"] is not None:\n",
        "    trainer.train_config.pitch_normalize = finetune_config[\"pitch_normalize\"]\n",
        "    trainer.train_config.pitch_normalize_conf[\"stats_file\"] = (\n",
        "        f\"{STATS_DIR}/train/pitch_stats.npz\"\n",
        "    )\n",
        "if finetune_config[\"energy_normalize\"] is not None:\n",
        "    trainer.train_config.energy_normalize = finetune_config[\"energy_normalize\"]\n",
        "    trainer.train_config.energy_normalize_conf[\"stats_file\"] = (\n",
        "        f\"{STATS_DIR}/train/energy_stats.npz\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjguYSIOuFIG"
      },
      "source": [
        "## Start Training\n",
        "\n",
        "Now, let's start the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOw7U1M4iI8N"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4phB7dFGxb_"
      },
      "source": [
        "# Inference\n",
        "\n",
        "When training is done, we can use the inference API to synthesize audio from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-54HF-WuFIH"
      },
      "outputs": [],
      "source": [
        "from espnet2.bin.tts_inference import inference\n",
        "\n",
        "ckpt_name = \"train.total_count.ave_10best\"\n",
        "inference_folder = f\"{EXP_DIR}/inference_{ckpt_name}\"\n",
        "model_file = f\"{EXP_DIR}/{ckpt_name}.pth\"\n",
        "\n",
        "inference(\n",
        "    output_dir=inference_folder,\n",
        "    batch_size=1,\n",
        "    dtype=\"float32\",\n",
        "    ngpu=0,\n",
        "    seed=0,\n",
        "    num_workers=1,\n",
        "    log_level=\"INFO\",\n",
        "    data_path_and_name_and_type=[\n",
        "        (f\"{DUMP_DIR}/raw/eval1/text\", \"text\", \"text\"),\n",
        "        (f\"{DUMP_DIR}/raw/eval1/wav.scp\", \"speech\", \"sound\"),\n",
        "        (f\"{DUMP_DIR}/xvector/eval1/xvector.scp\", \"spembs\", \"kaldi_ark\"),\n",
        "    ],\n",
        "    key_file=None,\n",
        "    train_config=f\"{EXP_DIR}/config.yaml\",\n",
        "    model_file=model_file,\n",
        "    model_tag=None,\n",
        "    threshold=0.5,\n",
        "    minlenratio=0.0,\n",
        "    maxlenratio=10.0,\n",
        "    use_teacher_forcing=False,\n",
        "    use_att_constraint=False,\n",
        "    backward_window=1,\n",
        "    forward_window=3,\n",
        "    speed_control_alpha=1.0,\n",
        "    noise_scale=0.667,\n",
        "    noise_scale_dur=0.8,\n",
        "    always_fix_seed=False,\n",
        "    allow_variable_data_keys=False,\n",
        "    vocoder_config=None,\n",
        "    vocoder_file=None,\n",
        "    vocoder_tag=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gbUgUZluFIH"
      },
      "source": [
        "# Evaluation\n",
        "In this section, we will assess the model's performance based on speaker similarity, Mel-cepstral distortion, the root mean square error (RMSE) of the fundamental frequency (f0), and the Pearson correlation coefficient for f0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiWnWhc5uFIH"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "from versa import speaker_metric, speaker_model_setup, mcd_f0\n",
        "\n",
        "gt_wav_scp = f\"{DUMP_DIR}/raw/eval1/wav.scp\"\n",
        "\n",
        "model = speaker_model_setup()\n",
        "spk_similarities = []\n",
        "mcd_f0s = []\n",
        "f0rmses = []\n",
        "f0corrs = []\n",
        "\n",
        "with open(gt_wav_scp, \"r\") as f:\n",
        "    for line in f:\n",
        "        key, path = line.strip().split()\n",
        "        gt, sr = sf.read(path)\n",
        "        pred, sr = sf.read(f\"{inference_folder}/wav/{key}.wav\")\n",
        "        ret = speaker_metric(model, pred, gt, sr)\n",
        "        with open(f\"{inference_folder}/spk_similarity\", \"a\") as f:\n",
        "            f.write(f\"{ret['spk_similarity']}\\n\")\n",
        "        spk_similarities.append(ret[\"spk_similarity\"])\n",
        "        ret = mcd_f0(pred, gt, sr, 1, 800, dtw=True)\n",
        "        with open(f\"{inference_folder}/mcd_f0\", \"a\") as f:\n",
        "            f.write(f\"{ret['mcd']}\\n\")\n",
        "        with open(f\"{inference_folder}/f0rmse\", \"a\") as f:\n",
        "            f.write(f\"{ret['f0rmse']}\\n\")\n",
        "        with open(f\"{inference_folder}/f0corr\", \"a\") as f:\n",
        "            f.write(f\"{ret['f0corr']}\\n\")\n",
        "        mcd_f0s.append(ret[\"mcd\"])\n",
        "        f0rmses.append(ret[\"f0rmse\"])\n",
        "        f0corrs.append(ret[\"f0corr\"])\n",
        "\n",
        "print(\"Averaged speaker similarity:\", sum(spk_similarities) / len(spk_similarities))\n",
        "print(\"Averaged MCD:\", sum(mcd_f0s) / len(mcd_f0s))\n",
        "print(\"Averaged F0 RMSE:\", sum(f0rmses) / len(f0rmses))\n",
        "print(\"Averaged F0 Corr:\", sum(f0corrs) / len(f0corrs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFVi8bdouFIH"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] S. Someki, K. Choi, S. Arora, W. Chen, S. Cornell, J. Han, Y. Peng, J. Shi, V. Srivastav, and S. Watanabe, “ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration,” *arXiv preprint* arXiv:2409.09506, 2024.\n",
        "\n",
        "[2] C. Veaux, J. Yamagishi, and K. MacDonald, “CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit,” University of Edinburgh, The Centre for Speech Technology Research (CSTR), 2017. [Sound]. https://doi.org/10.7488/ds/1994.\n",
        "\n",
        "[3] J. Shi, H. Shim, J. Tian, S. Arora, H. Wu, D. Petermann, J. Q. Yip, Y. Zhang, Y. Tang, W. Zhang, D. S. Alharthi, Y. Huang, K. Saito, J. Han, Y. Zhao, C. Donahue, and S. Watanabe, “VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music,” arXiv preprint arXiv:2412.17667, 2024."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}