{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HANJionghao/espnet_notebook/blob/master/ESPnetEZ/SVS/SVS_finetune_ace-kising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Bev3z9xk-6"
   },
   "source": [
    "# Fine-Tuning VISinger 2 for Singing Voice Synthesis on a New Dataset\n",
    "\n",
    "This Jupyter notebook provides a step-by-step guide on using the ESPnetEZ module to fine-tune a pretrained VISinger 2 model. In this demonstration, we will use ESPnet's singing corpora on Hugging Face for the SVS task. This demo covers data preparation, fine-tuning, inference, and evaluation.\n",
    "\n",
    "## Overview\n",
    "- Task: Singing Voice Synthesis\n",
    "- Dataset: [*ACE-KiSing*](https://huggingface.co/datasets/espnet/ace-kising-segments)\n",
    "- Model: *VISinger 2* model trained on *ACE-Opencpop* - [espnet/aceopencpop_svs_visinger2_40singer_pretrain](https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain)\n",
    "\n",
    "## License Reminder\n",
    "Before proceeding, please note that the datasets and models used in this tutorial come with specific licensing terms:\n",
    "\n",
    "- **ACE-KiSing Dataset:** The ACE-KiSing dataset is distributed under the Creative Commons Attribution Non Commercial 4.0 (CC BY-NC 4.0) license. This means you are free to use, share, and adapt the data, but only for non-commercial purposes. Any commercial use of this dataset is prohibited without explicit permission from the dataset creators.\n",
    "\n",
    "- **Pretrained VISinger 2 Model:** The VISinger 2 model used in this tutorial is distributed under the Creative Commons Attribution 4.0 (CC BY 4.0) license. This means you can use, modify, and redistribute the model, even for commercial purposes, as long as proper credit is given to the creators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7H4TMNw0kDz"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8laekV6fxk-7"
   },
   "outputs": [],
   "source": [
    "# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care\n",
    "# ESPnet installation\n",
    "!git clone --depth 5 https://github.com/espnet/espnet.git\n",
    "!cd espnet && pip install .\n",
    "\n",
    "!pip install espnet_model_zoo tensorboard\n",
    "\n",
    "!pip install pyopenjtalk==0.4\n",
    "!pip install pypinyin==0.44.0\n",
    "!pip install gdown==4.4.0\n",
    "!pip install ipywebrtc\n",
    "\n",
    "# Evaluation related\n",
    "!git clone --depth 5 https://github.com/shinjiwlab/versa.git\n",
    "!cd versa && pip install .\n",
    "!git clone https://github.com/ftshijt/versa_demo_egs.git\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# HuggingFace datasets to load ACE-KiSing singing voice dataset\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPjc2chaxk-8"
   },
   "source": [
    "## Import ESPnetEZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqnnrLTbxk-8"
   },
   "outputs": [],
   "source": [
    "import espnetez as ez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05rJsG030eS8"
   },
   "source": [
    "# Data Preparation\n",
    "We will use ESPnet's ACE-KiSing dataset available on Hugging Face: [espnet/ace-kising-segments](https://huggingface.co/datasets/espnet/ace-kising-segments). Let's begin by loading the dataset, resampling the audio to match the model's requirements, and wrapping it using ESPnetEZtaset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJgZ8xJKohw5"
   },
   "source": [
    "## Load dataset\n",
    "To start, load the ACE-KiSing dataset using the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRZUkK-4og8r"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"espnet/ace-kising-segments\", cache_dir=\"cache\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqKDni4zKLPy"
   },
   "source": [
    "Display the first two instances from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-QMh5dAKIOm"
   },
   "outputs": [],
   "source": [
    "it = iter(train_dataset)\n",
    "next(it), next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZIuMy0bVLp9"
   },
   "source": [
    "## Resample Audio\n",
    "Resample the audio to a 44.1kHz sampling rate to match the requirements of the pretrained model. For more details, refer to the model's [SVS configuration](https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain#svs-config).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-xBk_4AVR4P"
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=44100))\n",
    "valid_dataset = valid_dataset.cast_column(\"audio\", Audio(sampling_rate=44100))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax1UewXexk-9"
   },
   "source": [
    "## Define Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3P_f17Uxk--"
   },
   "outputs": [],
   "source": [
    "# Map from speaker names of the KiSing dataset to speaker ids matched with the pretrained model\n",
    "singer2sid = {\n",
    "    \"barber\": 3,\n",
    "    \"blanca\": 30,\n",
    "    \"changge\": 5,\n",
    "    \"chuci\": 19,\n",
    "    \"chuming\": 4,\n",
    "    \"crimson\": 1,\n",
    "    \"david\": 28,\n",
    "    \"ghost\": 27,\n",
    "    \"growl\": 25,\n",
    "    \"hiragi-yuki\": 22,\n",
    "    \"huolian\": 13,\n",
    "    \"kuro\": 2,\n",
    "    \"lien\": 29,\n",
    "    \"liyuan\": 9,\n",
    "    \"luanming\": 21,\n",
    "    \"luotianyi\": 31,\n",
    "    \"namine\": 8,\n",
    "    \"orange\": 12,\n",
    "    \"original\": 32,\n",
    "    \"qifu\": 16,\n",
    "    \"qili\": 15,\n",
    "    \"qixuan\": 7,\n",
    "    \"quehe\": 6,\n",
    "    \"ranhuhu\": 11,\n",
    "    \"steel\": 26,\n",
    "    \"tangerine\": 23,\n",
    "    \"tarara\": 20,\n",
    "    \"tuyuan\": 24,\n",
    "    \"wenli\": 10,\n",
    "    \"xiaomo\": 17,\n",
    "    \"xiaoye\": 14,\n",
    "    \"yanhe\": 33,\n",
    "    \"yuezhengling\": 34,\n",
    "    \"yunhao\": 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D6umH4wb7lF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define data mapping functions\n",
    "data_info = {\n",
    "    \"singing\": lambda d: d['audio']['array'].astype(np.float32),\n",
    "    \"score\": lambda d: (d['tempo'], list(zip(*[d[key] for key in ('note_start_times', 'note_end_times', 'note_lyrics', 'note_midi', 'note_phns')]))),\n",
    "    \"text\": lambda d: d['transcription'],\n",
    "    \"label\": lambda d: (np.array(list(zip(*[d[key] for key in ('phn_start_times', 'phn_end_times')]))), d['phns']),\n",
    "    \"sids\": lambda d: np.array([singer2sid[d['singer']]]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlTK97vmxk--"
   },
   "source": [
    "## Load as ESPnetEZ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efMtuDRexk--"
   },
   "outputs": [],
   "source": [
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "valid_dataset = ez.dataset.ESPnetEZDataset(valid_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMmUTSVVb7lG"
   },
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMFcKDhs7jLJ"
   },
   "source": [
    "## Download Pretrained VISinger 2 Model\n",
    "We'll use ESPnet's model zoo to download the [pretrained VISinger 2 model from the ACE-Opencpop dataset](https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z5szxVZJfr0"
   },
   "outputs": [],
   "source": [
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "\n",
    "PRETRAIN_MODEL = \"espnet/aceopencpop_svs_visinger2_40singer_pretrain\"\n",
    "d = ModelDownloader(cachedir=\"cache\")\n",
    "pretrain_downloaded = d.download_and_unpack(PRETRAIN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY6XrJp9xk--"
   },
   "source": [
    "## Configure Fine-Tuning\n",
    "Load the pretrained model's configuration and set it up for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfGw3xYh98Id"
   },
   "outputs": [],
   "source": [
    "TASK = \"gan_svs\"\n",
    "pretrain_config = ez.config.from_yaml(TASK, pretrain_downloaded[\"train_config\"])\n",
    "\n",
    "# Update the configuration with the downloaded model file path\n",
    "pretrain_config[\"model_file\"] = pretrain_downloaded[\"model_file\"]\n",
    "\n",
    "# Modify configuration for fine-tuning\n",
    "finetune_config = pretrain_config.copy()\n",
    "finetune_config[\"batch_size\"] = 1\n",
    "finetune_config[\"num_workers\"] = 1\n",
    "finetune_config[\"max_epoch\"] = 40\n",
    "finetune_config[\"save_lora_only\"] = False\n",
    "finetune_config[\"num_iters_per_epoch\"] = None\n",
    "finetune_config[\"use_ez_preprocessor\"] = True  # Use SVS preprocessor for loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPjWSrB78s0Z"
   },
   "source": [
    "## Initialize Trainer\n",
    "Define the trainer for the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMTHIKcb-McL"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"ace-kising\"\n",
    "EXP_DIR = f\"exp/finetune_{dataset_name}_{TASK}\"\n",
    "STATS_DIR = f\"exp/stats_{dataset_name}\"\n",
    "\n",
    "trainer = ez.Trainer(\n",
    "    task=TASK,\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    data_info=data_info,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUesA-3zxk-_"
   },
   "source": [
    "## Collect Statistics\n",
    "Before training, we need to collect data statistics (e.g., normalization stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q6jLEPOFp9O"
   },
   "outputs": [],
   "source": [
    "# Temporarily set to None, as we need to collect stats first\n",
    "trainer.train_config.normalize = None\n",
    "trainer.train_config.pitch_normalize = None\n",
    "trainer.train_config.energy_normalize = None\n",
    "\n",
    "# Collect stats\n",
    "trainer.collect_stats()\n",
    "\n",
    "# Restore normalization configs with collected stats\n",
    "trainer.train_config.normalize = finetune_config[\"normalize\"]\n",
    "trainer.train_config.pitch_normalize = finetune_config[\"pitch_normalize\"]\n",
    "trainer.train_config.normalize_conf[\"stats_file\"] = f\"{STATS_DIR}/train/feats_stats.npz\"\n",
    "trainer.train_config.pitch_normalize_conf[\"stats_file\"] = f\"{STATS_DIR}/train/pitch_stats.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jroayQlwb7lI"
   },
   "source": [
    "## Start Training\n",
    "Now, let's start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d40GeNWsb7lI"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUeu-y28b7lJ"
   },
   "source": [
    "# Inference\n",
    "Once the model is fine-tuned, you can generate synthesized singing voice using the test dataset.\n",
    "\n",
    "## Set Up the Model for Inference\n",
    "Load the trained model and prepare for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz3oSlKUAdS6"
   },
   "outputs": [],
   "source": [
    "from espnet2.bin.svs_inference import SingingGenerate\n",
    "\n",
    "ckpt_name = \"train.total_count.ave_10best\"\n",
    "m = SingingGenerate(\n",
    "    f\"{EXP_DIR}/config.yaml\",\n",
    "    f\"{EXP_DIR}/{ckpt_name}.pth\",\n",
    ")\n",
    "\n",
    "m.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWIVS2sPAeHA"
   },
   "source": [
    "## Wrap dataset with ESPnetEZDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMkcwLxOAln-"
   },
   "outputs": [],
   "source": [
    "test_dataset = ez.dataset.ESPnetEZDataset(test_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTbOx1BpAoJu"
   },
   "source": [
    "## Run inference with test data\n",
    "Here, we will demonstrate how to perform inference using a single data instance from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVTtK42Mb7lJ"
   },
   "outputs": [],
   "source": [
    "# Get the first instance\n",
    "(key, batch) = next(iter(test_dataset))\n",
    "\n",
    "# Remove unnecessary data from batch\n",
    "batch.pop(\"singing\")\n",
    "batch.pop(\"text\")\n",
    "sids = batch.pop(\"sids\")\n",
    "\n",
    "# Generate the output\n",
    "output_dict = m(batch, sids=sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6nwXah5BAru"
   },
   "source": [
    "Save the generated singing voice to a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daxS4r9Cqjt1"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write(\n",
    "    f\"{EXP_DIR}/{key}.wav\",\n",
    "    output_dict[\"wav\"].cpu().numpy(),\n",
    "    44100,\n",
    "    \"PCM_16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXJLhp7xk-_"
   },
   "source": [
    "# Evaluation\n",
    "In this section, we will assess the model's performance based on speaker similarity, Mel-cepstral distortion, the root mean square error (RMSE) of the fundamental frequency (f0), and the Pearson correlation coefficient for f0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSqh_2-Sxk-_"
   },
   "outputs": [],
   "source": [
    "from versa import speaker_metric, speaker_model_setup, mcd_f0\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_name = \"train.total_count.ave_10best\"\n",
    "sr = 44100\n",
    "\n",
    "EXP_DIR = Path(f\"exp/finetune_{dataset_name}_{TASK}\")\n",
    "inference_dir = EXP_DIR / f\"inference_test_{ckpt_name}\"\n",
    "\n",
    "(inference_dir / \"wav\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"espnet/ace-kising-segments\", cache_dir=\"cache\", split=\"test\"\n",
    ")\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=sr))\n",
    "test_dataset = ez.dataset.ESPnetEZDataset(test_dataset, data_info=data_info)\n",
    "loader = iter(test_dataset)\n",
    "\n",
    "\n",
    "model = speaker_model_setup()\n",
    "spk_similarities = []\n",
    "mcd_f0s = []\n",
    "f0rmses = []\n",
    "f0corrs = []\n",
    "for key, batch in loader:\n",
    "    gt = batch.pop(\"singing\")\n",
    "    sids = batch.pop(\"sids\")\n",
    "    batch.pop(\"text\")\n",
    "    output_dict = m(batch, sids=sids)\n",
    "    pred = output_dict[\"wav\"].cpu().numpy()\n",
    "    sf.write(\n",
    "        f\"{inference_dir}/wav/{key}.wav\",\n",
    "        pred,\n",
    "        sr,\n",
    "        \"PCM_16\",\n",
    "    )\n",
    "\n",
    "    ret = speaker_metric(model, pred, gt, sr)\n",
    "    with open(f\"{inference_dir}/spk_similarity\", \"a\") as f:\n",
    "        f.write(f\"{ret['spk_similarity']}\\n\")\n",
    "    spk_similarities.append(ret[\"spk_similarity\"])\n",
    "    ret = mcd_f0(pred, gt, sr, 1, 800, dtw=True)\n",
    "    with open(f\"{inference_dir}/mcd_f0\", \"a\") as f:\n",
    "        f.write(f\"{ret['mcd']}\\n\")\n",
    "    with open(f\"{inference_dir}/f0rmse\", \"a\") as f:\n",
    "        f.write(f\"{ret['f0rmse']}\\n\")\n",
    "    with open(f\"{inference_dir}/f0corr\", \"a\") as f:\n",
    "        f.write(f\"{ret['f0corr']}\\n\")\n",
    "    mcd_f0s.append(ret[\"mcd\"])\n",
    "    f0rmses.append(ret[\"f0rmse\"])\n",
    "    f0corrs.append(ret[\"f0corr\"])\n",
    "\n",
    "print(\"Averaged speaker similarity:\", sum(spk_similarities) / len(spk_similarities))\n",
    "print(\"Averaged MCD:\", sum(mcd_f0s) / len(mcd_f0s))\n",
    "print(\"Averaged F0 RMSE:\", sum(f0rmses) / len(f0rmses))\n",
    "print(\"Averaged F0 Corr:\", sum(f0corrs) / len(f0corrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTaprLZzxk-_"
   },
   "source": [
    "# References\n",
    "[1] S. Someki, K. Choi, S. Arora, W. Chen, S. Cornell, J. Han, Y. Peng, J. Shi, V. Srivastav, and S. Watanabe, “ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration,” *arXiv preprint* arXiv:2409.09506, 2024.\n",
    "\n",
    "[2] J. Shi, Y. Lin, X. Bai, K. Zhang, Y. Wu, Y. Tang, Y. Yu, Q. Jin, and S. Watanabe, “Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing,” *arXiv preprint* arXiv:2401.17619, 2024.\n",
    "\n",
    "[3] J. Shi, H. Shim, J. Tian, S. Arora, H. Wu, D. Petermann, J. Q. Yip, Y. Zhang, Y. Tang, W. Zhang, D. S. Alharthi, Y. Huang, K. Saito, J. Han, Y. Zhao, C. Donahue, and S. Watanabe, “VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music,” arXiv preprint arXiv:2412.17667, 2024."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "p4rWUVGA03XT"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
