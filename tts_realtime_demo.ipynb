{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuhqhYSToxl7"
   },
   "source": [
    "# ESPnet real time E2E-TTS demonstration\n",
    "\n",
    "This notebook provides demonstration of realtime E2E-TTS using ESPnet-TTS and ParallelWaveGAN.\n",
    "\n",
    "- ESPnet: https://github.com/espnet/espnet\n",
    "- ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\n",
    "\n",
    "Author: Tomoki Hayashi (@kan-bayashi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9e_i_gdgAFNJ"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjJ5zkyaoy29"
   },
   "outputs": [],
   "source": [
    "# install minimal components\n",
    "!pip install -q parallel_wavegan PyYaml unidecode ConfigArgparse g2p_en nltk\n",
    "!git clone -q https://github.com/espnet/espnet.git\n",
    "!cd espnet && git fetch && git checkout -b v.0.6.0 8bfb7ac6974699e9720558a4ef20376805e38d6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lMJyJcLCsd4"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "## English demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1a5CgX1AHXJ"
   },
   "source": [
    "### Download pretrained models\n",
    "\n",
    "You can select one from three models. Please only run the seletected model cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWaOkhGVQNla"
   },
   "source": [
    "#### (a) Tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCM9Eo2cPXhZ"
   },
   "outputs": [],
   "source": [
    "# download char based Tacotron2\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7 downloads/ tar.gz > /dev/null 2>&1\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1hiZn14ITUDM1nkn-GkaN_M3oaTOUcn1n downloads/ tar.gz > /dev/null 2>&1\n",
    "\n",
    "# set path\n",
    "trans_type = \"char\"\n",
    "dict_path = \"downloads/data/lang_1char/train_no_dev_units.txt\"\n",
    "model_path = \"downloads/exp/train_no_dev_pytorch_train_pytorch_tacotron2.v3/results/model.last1.avg.best\"\n",
    "vocoder_path = \"downloads/ljspeech.parallel_wavegan.v1/checkpoint-400000steps.pkl\"\n",
    "vocoder_conf = \"downloads/ljspeech.parallel_wavegan.v1/config.yml\"\n",
    "print(\"sucessfully finished download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6dhfhcrQI6_"
   },
   "source": [
    "#### (b) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztWZjy_XOPZR"
   },
   "outputs": [],
   "source": [
    "# download phoneme based Transformer\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7 downloads/ tar.gz > /dev/null 2>&1\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1M_w7nxI6AfbtSHpMO-exILnAc_aUYvXP downloads/ tar.gz > /dev/null 2>&1\n",
    "\n",
    "# set path\n",
    "trans_type = \"phn\"\n",
    "dict_path = \"downloads/data/lang_1phn/train_no_dev_units.txt\"\n",
    "model_path = \"downloads/exp/phn_train_no_dev_pytorch_train_pytorch_transformer.v3/results/model.last1.avg.best\"\n",
    "vocoder_path = \"downloads/ljspeech.parallel_wavegan.v1/checkpoint-400000steps.pkl\"\n",
    "vocoder_conf = \"downloads/ljspeech.parallel_wavegan.v1/config.yml\"\n",
    "print(\"sucessfully finished download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VONMvIC1ODvR"
   },
   "source": [
    "#### (c) FastSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX0Kmo72POfY"
   },
   "outputs": [],
   "source": [
    "# download phoneme based FastSpeech\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7 downloads/ tar.gz > /dev/null 2>&1\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1otwFFYiVMcbbgY55xk6DrOfb8Pi5uqjJ downloads/ tar.gz > /dev/null 2>&1\n",
    "\n",
    "# set path\n",
    "trans_type = \"phn\"\n",
    "dict_path = \"downloads/data/lang_1phn/train_no_dev_units.txt\"\n",
    "model_path = \"downloads/exp/phn_train_no_dev_pytorch_train_fastspeech.v4/results/model.last1.avg.best\"\n",
    "vocoder_path = \"downloads/ljspeech.parallel_wavegan.v1/checkpoint-400000steps.pkl\"\n",
    "vocoder_conf = \"downloads/ljspeech.parallel_wavegan.v1/config.yml\"\n",
    "print(\"sucessfully finished download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaSyEKBWAK7H"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8JXOfRfqMFN"
   },
   "outputs": [],
   "source": [
    "# add path\n",
    "import sys\n",
    "sys.path.append(\"espnet/egs/ljspeech/tts1/local\")\n",
    "sys.path.append(\"espnet\")\n",
    "\n",
    "# define device\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# define E2E-TTS model\n",
    "from argparse import Namespace\n",
    "from espnet.asr.asr_utils import get_model_conf\n",
    "from espnet.asr.asr_utils import torch_load\n",
    "from espnet.utils.dynamic_import import dynamic_import\n",
    "idim, odim, train_args = get_model_conf(model_path)\n",
    "model_class = dynamic_import(train_args.model_module)\n",
    "model = model_class(idim, odim, train_args)\n",
    "torch_load(model_path, model)\n",
    "model = model.eval().to(device)\n",
    "inference_args = Namespace(**{\"threshold\": 0.5, \"minlenratio\": 0.0, \"maxlenratio\": 10.0})\n",
    "\n",
    "# define neural vocoders\n",
    "import yaml\n",
    "from parallel_wavegan.models import ParallelWaveGANGenerator\n",
    "with open(vocoder_conf) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "vocoder = ParallelWaveGANGenerator(**config[\"generator_params\"])\n",
    "vocoder.load_state_dict(torch.load(vocoder_path, map_location=\"cpu\")[\"model\"][\"generator\"])\n",
    "vocoder.remove_weight_norm()\n",
    "vocoder = vocoder.eval().to(device)\n",
    "\n",
    "# define text frontend\n",
    "from text.cleaners import custom_english_cleaners\n",
    "from g2p_en import G2p\n",
    "with open(dict_path) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.replace(\"\\n\", \"\").split(\" \") for line in lines]\n",
    "char_to_id = {c: int(i) for c, i in lines}\n",
    "g2p = G2p()\n",
    "def frontend(text):\n",
    "    \"\"\"Clean text and then convert to id sequence.\"\"\"\n",
    "    text = custom_english_cleaners(text)\n",
    "    \n",
    "    if trans_type == \"phn\":\n",
    "        text = filter(lambda s: s != \" \", g2p(text))\n",
    "        text = \" \".join(text)\n",
    "        print(f\"Cleaned text: {text}\")\n",
    "        charseq = text.split(\" \")\n",
    "    else:\n",
    "        print(f\"Cleaned text: {text}\")\n",
    "        charseq = list(text)\n",
    "    idseq = []\n",
    "    for c in charseq:\n",
    "        if c.isspace():\n",
    "            idseq += [char_to_id[\"<space>\"]]\n",
    "        elif c not in char_to_id.keys():\n",
    "            idseq += [char_to_id[\"<unk>\"]]\n",
    "        else:\n",
    "            idseq += [char_to_id[c]]\n",
    "    idseq += [idim - 1]  # <eos>\n",
    "    return torch.LongTensor(idseq).view(-1).to(device)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "print(\"Now ready to synthesize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AacD_RerASiO"
   },
   "source": [
    "### Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gGRzrjyudWF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Input your favorite sentencne in English!\")\n",
    "input_text = input()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    x = frontend(input_text)\n",
    "    c, _, _ = model.inference(x, inference_args)\n",
    "    z = torch.randn(1, 1, c.size(0) * config[\"hop_size\"]).to(device)\n",
    "    c = torch.nn.ReplicationPad1d(\n",
    "        config[\"generator_params\"][\"aux_context_window\"])(c.unsqueeze(0).transpose(2, 1))\n",
    "    y = vocoder(z, c).view(-1)\n",
    "rtf = (time.time() - start) / (len(y) / config[\"sampling_rate\"])\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(y.view(-1).cpu().numpy(), rate=config[\"sampling_rate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtSZpF-mCjTr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Japanese demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOkxcmprLYD8"
   },
   "source": [
    "### Install Japanese dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpHnzqesEMfh"
   },
   "outputs": [],
   "source": [
    "# install dependency\n",
    "!mkdir tools && cd tools && git clone https://github.com/r9y9/hts_engine_API.git\n",
    "!cd tools/hts_engine_API/src && ./waf configure && ./waf build install\n",
    "!cd tools && git clone https://github.com/r9y9/open_jtalk.git\n",
    "!mkdir -p tools/open_jtalk/src/build && cd tools/open_jtalk/src/build && cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. && make install\n",
    "!cp tools/open_jtalk/src/build/*.so* /usr/lib64-nvidia\n",
    "!cd tools && git clone https://github.com/r9y9/pyopenjtalk.git\n",
    "!cd tools/pyopenjtalk && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQiWSgwULc9L"
   },
   "source": [
    "### Download pretrained models\n",
    "\n",
    "Here we select Tacotron2 or Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnWn46FmF8Nv"
   },
   "source": [
    "#### (a) Tacotron 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBEWUGItF2hf"
   },
   "outputs": [],
   "source": [
    "# download pretrained models\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM downloads tar.gz > /dev/null 2>&1\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1kp5M4VvmagDmYckFJa78WGqh1drb_P9t downloads tar.gz > /dev/null 2>&1\n",
    "\n",
    "# set path\n",
    "dict_path = \"downloads/data/lang_1phn/train_no_dev_units.txt\"\n",
    "model_path = \"downloads/exp/train_no_dev_pytorch_train_pytorch_tacotron2_phn/results/model.last1.avg.best\"\n",
    "vocoder_path = \"downloads/jsut.parallel_wavegan.v1/checkpoint-400000steps.pkl\"\n",
    "vocoder_conf = \"downloads/jsut.parallel_wavegan.v1/config.yml\"\n",
    "\n",
    "print(\"sucessfully finished download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta6wNJ4WGAEP"
   },
   "source": [
    "#### (b) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7fLzr99CogD"
   },
   "outputs": [],
   "source": [
    "# download pretrained models\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM downloads tar.gz > /dev/null 2>&1\n",
    "!./espnet/utils/download_from_google_drive.sh \\\n",
    "    https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD downloads tar.gz > /dev/null 2>&1\n",
    "\n",
    "# set path\n",
    "dict_path = \"downloads/data/lang_1phn/train_no_dev_units.txt\"\n",
    "model_path = \"downloads/exp/train_no_dev_pytorch_train_pytorch_transformer_phn/results/model.last1.avg.best\"\n",
    "vocoder_path = \"downloads/jsut.parallel_wavegan.v1/checkpoint-400000steps.pkl\"\n",
    "vocoder_conf = \"downloads/jsut.parallel_wavegan.v1/config.yml\"\n",
    "\n",
    "print(\"sucessfully finished download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7O2FXi1uLrUV"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69vGlN12DqB2"
   },
   "outputs": [],
   "source": [
    "# add path\n",
    "import sys\n",
    "sys.path.append(\"espnet/egs/ljspeech/tts1/local\")\n",
    "sys.path.append(\"espnet\")\n",
    "\n",
    "# define device\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# define E2E-TTS model\n",
    "from argparse import Namespace\n",
    "from espnet.asr.asr_utils import get_model_conf\n",
    "from espnet.asr.asr_utils import torch_load\n",
    "from espnet.utils.dynamic_import import dynamic_import\n",
    "idim, odim, train_args = get_model_conf(model_path)\n",
    "model_class = dynamic_import(train_args.model_module)\n",
    "model = model_class(idim, odim, train_args)\n",
    "torch_load(model_path, model)\n",
    "model = model.eval().to(device)\n",
    "inference_args = Namespace(**{\"threshold\": 0.3, \"minlenratio\": 0.0, \"maxlenratio\": 10.0})\n",
    "\n",
    "# define neural vocoders\n",
    "import yaml\n",
    "from parallel_wavegan.models import ParallelWaveGANGenerator\n",
    "with open(vocoder_conf) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "vocoder = ParallelWaveGANGenerator(**config[\"generator_params\"])\n",
    "vocoder.load_state_dict(torch.load(vocoder_path, map_location=\"cpu\")[\"model\"][\"generator\"])\n",
    "vocoder.remove_weight_norm()\n",
    "vocoder = vocoder.eval().to(device)\n",
    "\n",
    "# define text frontend\n",
    "import pyopenjtalk\n",
    "with open(dict_path) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.replace(\"\\n\", \"\").split(\" \") for line in lines]\n",
    "char_to_id = {c: int(i) for c, i in lines}\n",
    "def frontend(text):\n",
    "    \"\"\"Clean text and then convert to id sequence.\"\"\"\n",
    "    text = pyopenjtalk.g2p(text, kana=False)\n",
    "    print(f\"Cleaned text: {text}\")\n",
    "    charseq = text.split(\" \")\n",
    "    idseq = []\n",
    "    for c in charseq:\n",
    "        if c.isspace():\n",
    "            idseq += [char_to_id[\"<space>\"]]\n",
    "        elif c not in char_to_id.keys():\n",
    "            idseq += [char_to_id[\"<unk>\"]]\n",
    "        else:\n",
    "            idseq += [char_to_id[c]]\n",
    "    idseq += [idim - 1]  # <eos>\n",
    "    return torch.LongTensor(idseq).view(-1).to(device)\n",
    "\n",
    "frontend(\"初回の辞書のインストールが必要です\")\n",
    "print(\"Now ready to synthesize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmyyM1RCN1Rs"
   },
   "source": [
    "### Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2Dk9o0-JlbN"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"日本語で好きな文章を入力してください\")\n",
    "input_text = input()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    x = frontend(input_text)\n",
    "    c, _, _ = model.inference(x, inference_args)\n",
    "    z = torch.randn(1, 1, c.size(0) * config[\"hop_size\"]).to(device)\n",
    "    c = torch.nn.ReplicationPad1d(config[\"generator_params\"][\"aux_context_window\"])(c.unsqueeze(0).transpose(2, 1))\n",
    "    y = vocoder(z, c).view(-1)\n",
    "rtf = (time.time() - start) / (len(y) / config[\"sampling_rate\"])\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(y.view(-1).cpu().numpy(), rate=config[\"sampling_rate\"]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "E2E-TTS demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
